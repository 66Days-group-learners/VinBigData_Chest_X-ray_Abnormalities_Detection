{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "AWS_server_download_1024_dataset.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ranking-template"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook run on AWS Ubuntu 18.04 Deep Learning server t2.xlarge instance.\n",
        "The goal is to download the dataset from Kaggle or Dropbox."
      ],
      "id": "ranking-template"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "higher-permit"
      },
      "source": [
        "# Initialize"
      ],
      "id": "higher-permit"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cloudy-reproduction"
      },
      "source": [
        "# Run this command if needed to erase data of previous runs\n",
        "#!rm -rf /home/ubuntu/GitHub\n",
        "#!rm -rf ~/.kaggle"
      ],
      "id": "cloudy-reproduction",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "second-monroe",
        "outputId": "cac09395-8724-483e-cad6-0d39ddfef6ed"
      },
      "source": [
        "!mkdir /home/ubuntu/GitHub\n",
        "!mkdir /home/ubuntu/GitHub/Data\n",
        "!mkdir /home/ubuntu/GitHub/Data/vinbigdata"
      ],
      "id": "second-monroe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/home/ubuntu/GitHub’: File exists\n",
            "mkdir: cannot create directory ‘/home/ubuntu/GitHub/Data’: File exists\n",
            "mkdir: cannot create directory ‘/home/ubuntu/GitHub/Data/vinbigdata’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sealed-scene",
        "outputId": "4781631b-d429-49e6-d7c9-eb766795b095"
      },
      "source": [
        "!pip3 install kaggle"
      ],
      "id": "sealed-scene",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in ./miniconda3/envs/ml/lib/python3.7/site-packages (1.5.12)\r\n",
            "Requirement already satisfied: urllib3 in ./miniconda3/envs/ml/lib/python3.7/site-packages (from kaggle) (1.26.4)\r\n",
            "Requirement already satisfied: certifi in ./miniconda3/envs/ml/lib/python3.7/site-packages (from kaggle) (2020.12.5)\r\n",
            "Requirement already satisfied: six>=1.10 in ./miniconda3/envs/ml/lib/python3.7/site-packages (from kaggle) (1.15.0)\r\n",
            "Requirement already satisfied: requests in ./miniconda3/envs/ml/lib/python3.7/site-packages (from kaggle) (2.25.1)\r\n",
            "Requirement already satisfied: python-dateutil in ./miniconda3/envs/ml/lib/python3.7/site-packages (from kaggle) (2.8.1)\r\n",
            "Requirement already satisfied: tqdm in ./miniconda3/envs/ml/lib/python3.7/site-packages (from kaggle) (4.59.0)\r\n",
            "Requirement already satisfied: python-slugify in ./miniconda3/envs/ml/lib/python3.7/site-packages (from kaggle) (4.0.1)\r\n",
            "Requirement already satisfied: text-unidecode>=1.3 in ./miniconda3/envs/ml/lib/python3.7/site-packages (from python-slugify->kaggle) (1.3)\r\n",
            "Requirement already satisfied: idna<3,>=2.5 in ./miniconda3/envs/ml/lib/python3.7/site-packages (from requests->kaggle) (2.10)\r\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in ./miniconda3/envs/ml/lib/python3.7/site-packages (from requests->kaggle) (4.0.0)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mounted-webster"
      },
      "source": [
        "## Download my Kaggle token"
      ],
      "id": "mounted-webster"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hindu-stake",
        "outputId": "30d0f08d-ccad-4b96-b5ce-e6cdb9d0c946"
      },
      "source": [
        "!mkdir ~/.kaggle"
      ],
      "id": "hindu-stake",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/home/ubuntu/.kaggle’: File exists\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "further-compact"
      },
      "source": [
        "# Download my json token in order to acces the 1080 dataset using the kaggle API\n",
        "# See\n",
        "# http://www.xavierdupre.fr/blog/2015-01-20_nojs.html\n",
        "# https://kobkrit.com/how-to-directly-download-files-from-dropbox-or-google-drive-using-wget-in-terminal-or-in-google-573168195011\n",
        "# https://github.com/Kaggle/kaggle-api\n",
        "import os\n",
        "os.chdir('/home/ubuntu/.kaggle')\n",
        "import urllib.request\n",
        "url = 'adress_to_my_token' # Changing the link to dl=1 is required to allow download\n",
        "\n",
        "u = urllib.request.urlopen(url)\n",
        "data = u.read()\n",
        "u.close()\n",
        " \n",
        "with open('kaggle.json', \"wb\") as f :\n",
        "    f.write(data)    "
      ],
      "id": "further-compact",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "identified-count"
      },
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "id": "identified-count",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "structured-sculpture"
      },
      "source": [
        "# Download the 1024 dataset"
      ],
      "id": "structured-sculpture"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rental-cabinet"
      },
      "source": [
        "# https://www.kaggle.com/awsaf49/vinbigdata-1024-image-dataset\n",
        "\n",
        "os.chdir('/home/ubuntu/GitHub/Data/vinbigdata')\n",
        "\n",
        "import kaggle\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "\n",
        "api.dataset_download_files('awsaf49/vinbigdata-1024-image-dataset', path='./')"
      ],
      "id": "rental-cabinet",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piano-filing"
      },
      "source": [
        "The download was fast using this method. It took less than 40 min to download. But The disk is full! A lot of the 100 GB might be already in use on those Deep Learming instance?"
      ],
      "id": "piano-filing"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suffering-manitoba"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/home/ubuntu/GitHub/Data/vinbigdata/vinbigdata-1024-image-dataset.zip', 'r') as zipref:\n",
        "    zipref.extractall('/home/ubuntu/GitHub/Data/vinbigdata')"
      ],
      "id": "suffering-manitoba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gross-framework"
      },
      "source": [
        "# Downoload the label"
      ],
      "id": "gross-framework"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brilliant-sculpture"
      },
      "source": [
        "os.chdir('/home/ubuntu/GitHub/Data/')\n",
        "\n",
        "url = 'https://www.dropbox.com/s/wcvp0dshzvfv5gl/labels.zip?dl=1' # Changing the link to dl=1 is required to allow download\n",
        "\n",
        "u = urllib.request.urlopen(url)\n",
        "data = u.read()\n",
        "u.close()\n",
        " \n",
        "with open('label.zip', \"wb\") as f :\n",
        "    f.write(data)    "
      ],
      "id": "brilliant-sculpture",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "revolutionary-scale"
      },
      "source": [
        "with zipfile.ZipFile('/home/ubuntu/GitHub/Data/label.zip', 'r') as zipref:\n",
        "    zipref.extractall('/home/ubuntu/GitHub/Data/')"
      ],
      "id": "revolutionary-scale",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "built-yukon"
      },
      "source": [
        "# References"
      ],
      "id": "built-yukon"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "proper-sodium"
      },
      "source": [
        "* https://www.codespeedy.com/how-to-download-files-from-url-using-python/"
      ],
      "id": "proper-sodium"
    }
  ]
}